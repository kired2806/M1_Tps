{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT DES LIBRAIRIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT DU DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-264d3c610226>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'comments_students.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/salem/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/salem/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/salem/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nrows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/salem/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/salem/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m     \"\"\"\n\u001b[1;32m    574\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('comments_students.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>name</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1430438400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34f9rh</td>\n",
       "      <td>t1_cqug90j</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug90j</td>\n",
       "      <td>jesse9o3</td>\n",
       "      <td>No one has a European accent either  because i...</td>\n",
       "      <td>t1_cqug2sr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1430438400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34fvry</td>\n",
       "      <td>t1_cqug90k</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug90k</td>\n",
       "      <td>beltfedshooter</td>\n",
       "      <td>That the kid ..reminds me of Kevin.   so sad :-(</td>\n",
       "      <td>t3_34fvry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1430438400</td>\n",
       "      <td>5.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34ffo5</td>\n",
       "      <td>t1_cqug90z</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug90z</td>\n",
       "      <td>InterimFatGuy</td>\n",
       "      <td>NSFL</td>\n",
       "      <td>t1_cqu80zb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1430438401</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34aqsn</td>\n",
       "      <td>t1_cqug91c</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug91c</td>\n",
       "      <td>JuanTutrego</td>\n",
       "      <td>I'm a guy and I had no idea this was a thing g...</td>\n",
       "      <td>t1_cqtdj4m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1430438401</td>\n",
       "      <td>101.0</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_34f9rh</td>\n",
       "      <td>t1_cqug91e</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>cqug91e</td>\n",
       "      <td>dcblackbelt</td>\n",
       "      <td>Mid twenties male rocking skinny jeans/pants, ...</td>\n",
       "      <td>t1_cquc4rc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0 created_utc    ups subreddit_id    link_id        name  \\\n",
       "0          0  1430438400    3.0     t5_2qh1i  t3_34f9rh  t1_cqug90j   \n",
       "1          1  1430438400    3.0     t5_2qh1i  t3_34fvry  t1_cqug90k   \n",
       "2          2  1430438400    5.0     t5_2qh1i  t3_34ffo5  t1_cqug90z   \n",
       "3          3  1430438401    1.0     t5_2qh1i  t3_34aqsn  t1_cqug91c   \n",
       "4          4  1430438401  101.0     t5_2qh1i  t3_34f9rh  t1_cqug91e   \n",
       "\n",
       "   subreddit       id          author  \\\n",
       "0  AskReddit  cqug90j        jesse9o3   \n",
       "1  AskReddit  cqug90k  beltfedshooter   \n",
       "2  AskReddit  cqug90z   InterimFatGuy   \n",
       "3  AskReddit  cqug91c     JuanTutrego   \n",
       "4  AskReddit  cqug91e     dcblackbelt   \n",
       "\n",
       "                                                body   parent_id  \n",
       "0  No one has a European accent either  because i...  t1_cqug2sr  \n",
       "1   That the kid ..reminds me of Kevin.   so sad :-(   t3_34fvry  \n",
       "2                                               NSFL  t1_cqu80zb  \n",
       "3  I'm a guy and I had no idea this was a thing g...  t1_cqtdj4m  \n",
       "4  Mid twenties male rocking skinny jeans/pants, ...  t1_cquc4rc  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAVAUX PRELIMINAIRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUPRESSION DES COLONNES INUTILES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppresion de colonnes inexploitable par notre modèle\n",
    "# subreddit et subreddit id : les individus sont tous identiques ou tous différents\n",
    "# author : apportera du bruit sur notre modèle si on applique une prédiction avec un author inconnu\n",
    "df = df.drop(['subreddit', 'subreddit_id', 'author'], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>link_id</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1430438400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>t3_34f9rh</td>\n",
       "      <td>t1_cqug90j</td>\n",
       "      <td>cqug90j</td>\n",
       "      <td>No one has a European accent either  because i...</td>\n",
       "      <td>t1_cqug2sr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1430438400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>t3_34fvry</td>\n",
       "      <td>t1_cqug90k</td>\n",
       "      <td>cqug90k</td>\n",
       "      <td>That the kid ..reminds me of Kevin.   so sad :-(</td>\n",
       "      <td>t3_34fvry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1430438400</td>\n",
       "      <td>5.0</td>\n",
       "      <td>t3_34ffo5</td>\n",
       "      <td>t1_cqug90z</td>\n",
       "      <td>cqug90z</td>\n",
       "      <td>NSFL</td>\n",
       "      <td>t1_cqu80zb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1430438401</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t3_34aqsn</td>\n",
       "      <td>t1_cqug91c</td>\n",
       "      <td>cqug91c</td>\n",
       "      <td>I'm a guy and I had no idea this was a thing g...</td>\n",
       "      <td>t1_cqtdj4m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1430438401</td>\n",
       "      <td>101.0</td>\n",
       "      <td>t3_34f9rh</td>\n",
       "      <td>t1_cqug91e</td>\n",
       "      <td>cqug91e</td>\n",
       "      <td>Mid twenties male rocking skinny jeans/pants, ...</td>\n",
       "      <td>t1_cquc4rc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0 created_utc    ups    link_id        name       id  \\\n",
       "0          0  1430438400    3.0  t3_34f9rh  t1_cqug90j  cqug90j   \n",
       "1          1  1430438400    3.0  t3_34fvry  t1_cqug90k  cqug90k   \n",
       "2          2  1430438400    5.0  t3_34ffo5  t1_cqug90z  cqug90z   \n",
       "3          3  1430438401    1.0  t3_34aqsn  t1_cqug91c  cqug91c   \n",
       "4          4  1430438401  101.0  t3_34f9rh  t1_cqug91e  cqug91e   \n",
       "\n",
       "                                                body   parent_id  \n",
       "0  No one has a European accent either  because i...  t1_cqug2sr  \n",
       "1   That the kid ..reminds me of Kevin.   so sad :-(   t3_34fvry  \n",
       "2                                               NSFL  t1_cqu80zb  \n",
       "3  I'm a guy and I had no idea this was a thing g...  t1_cqtdj4m  \n",
       "4  Mid twenties male rocking skinny jeans/pants, ...  t1_cquc4rc  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NETTOYAGE DU CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du data frame rempli avec les corpus de texte\n",
    "df_txt = pd.DataFrame(columns = ['corpus'])\n",
    "df_txt['corpus'] = df['body']\n",
    "df_txt = df_txt.astype({'corpus': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    one european accent either doesnt exist accent...\n",
       "1                                kid reminds kevin sad\n",
       "2                                                 nsfl\n",
       "3                                im guy idea thing guy\n",
       "4    mid twenty male rocking skinny jeanspants styl...\n",
       "Name: corpus, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# création d'un nouveau dataframe\n",
    "df_txt_clean = pd.DataFrame()\n",
    "\n",
    "# supression de la casse\n",
    "df_txt_clean['corpus'] = df_txt['corpus'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# suppresion des caractère spéciaux\n",
    "df_txt_clean['corpus'] = df_txt_clean['corpus'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# suppression les stops words\n",
    "stop = stopwords.words('english')\n",
    "df_txt_clean['corpus'] = df_txt_clean['corpus'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# suppression des mots communs (on peut changer valeur)\n",
    "# n'est pas utilisé car on appliquera ce dispositif lors de la création du modèle tf-idf par exemple\n",
    "\"\"\"\n",
    "#freq = pd.Series(' '.join(df_txt_clean['corpus']).split()).value_counts()[:100]\n",
    "#freq = list(freq.index)\n",
    "#df_txt_clean['corpus'] = df_txt_clean['corpus'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\"\"\"\n",
    "\n",
    "# suppression des mots rares (on peut changer valeur)\n",
    "# n'est pas utilisé car on appliquera ce dispositif lors de la création du modèle tf-idf par exemple\n",
    "\"\"\"\n",
    "freq = pd.Series(' '.join(df_txt_clean['corpus']).split()).value_counts()[-10000:]\n",
    "freq = list(freq.index)\n",
    "df_txt_clean['corpus'] = df_txt_clean['corpus'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\"\"\"\n",
    "\n",
    "# lemmatization\n",
    "df_txt_clean['corpus'] = df_txt_clean['corpus'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "# stemming\n",
    "# n'est pas utilisé car la lemmatization est plus performant sur notre problème (elle regroupe plus de mots)\n",
    "\"\"\"\n",
    "st = PorterStemmer()\n",
    "df_txt_clean['corpus'] = df_txt_clean['corpus'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\"\"\"\n",
    "df_txt_clean['corpus'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_txt_clean.to_csv('./txt_clean.csv', index=False)\n",
    "# df_txt_clean = pd.read_csv('txt_clean.csv')\n",
    "# df_txt_clean = df_txt_clean.astype({'corpus': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sauvegarde et chargement du texte nettoyé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATION DU RESEAUX NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.DiGraph()\n",
    "g.add_nodes_from(df.link_id, type = \"link\")\n",
    "g.add_nodes_from(df.name, type = \"comment\")\n",
    "g.add_edges_from(df[[\"name\", \"parent_id\"]].values) # , link_type = \"parent\")\n",
    "# nx.write_gml(g,\"graph.gml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = nx.read_gml(\"graph.gml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATION DES FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATION DE LA TABLE AVEC FEATURES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dataframe qui sera utilisé plus tard pour entraîner le modèle\n",
    "# df_feat a une colonne 'ups' plus une colonne pour chaque feature\n",
    "g_nodes = dict(g.in_degree())\n",
    "\n",
    "# df = pd.DataFrame.from_dict(g_nodes, orient='index')\n",
    "serie = pd.Series(g_nodes)\n",
    " \n",
    "# sns.distplot(serie, bins = 20, kde=False) # cannot use dataframe with displot\n",
    "df_feat = pd.DataFrame.from_dict(g_nodes, orient='index')\n",
    "df_feat = df_feat.reset_index()\n",
    "\n",
    "# on sélectionne uniquement les noeuds qui sont présents dans le dataset\n",
    "df_feat = df_feat[df_feat['index'].isin(list(df['name']))] \n",
    "df_feat = df_feat.drop('index',axis=1)\n",
    "df_feat.rename(columns={0: 'entry_nodes'}, inplace=True)\n",
    "df_feat = df_feat.reset_index(drop=True)\n",
    "df = df.reset_index(drop=True)\n",
    "df_feat['ups'] = df['ups']\n",
    "del df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature avec corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nombre de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txt['word_count'] = df_txt['corpus'].apply(lambda x: len(str(x). split(\" \")))\n",
    "df_feat['word_count'] = list(df_txt['word_count'])\n",
    "df_txt[['corpus','word_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nombre de charactères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txt['char_count'] = df_txt['corpus'].str.len()\n",
    "df_feat['char_count'] = list(df_txt['char_count'])\n",
    "df_txt[['corpus', 'char_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### longueur moyenne des mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    if len(words) == 0 :\n",
    "        return 1\n",
    "    else :\n",
    "        return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "df_txt['avg_word'] = df_txt['corpus'].apply(lambda x: avg_word(x))\n",
    "df_feat['avg_word'] = list(df_txt['avg_word'])\n",
    "df_txt[['corpus','avg_word']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nombre de stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "df_txt['stopwords'] = df_txt['corpus'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "df_feat['stopwords'] = list(df_txt['stopwords'])\n",
    "df_txt[['corpus','stopwords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nombre de charactères numérique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txt['numerics'] = df_txt['corpus'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "df_feat['numerics'] = list(df_txt['numerics'])\n",
    "df_txt[['corpus','numerics']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nombre de mots en majuscules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txt['upper'] = df_txt['corpus'].apply(lambda x: len([x for x in x.split() if x.isupper() and x != \"I\"]))\n",
    "df_feat['upper'] = list(df_txt['upper'])\n",
    "df_txt[['corpus','upper']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiments Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_txt['sentiment_polarity'] = df_txt_clean['corpus'].apply(lambda x: TextBlob(x).sentiment[0])\n",
    "df_txt['sentiment_subjectivity'] = df_txt_clean['corpus'].apply(lambda x: TextBlob(x).sentiment[1])\n",
    "df_txt[['sentiment_polarity', 'sentiment_subjectivity']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sentiment = df_txt[['sentiment_polarity', 'sentiment_subjectivity']]\n",
    "#df_sentiment.to_csv('/home/cmisid/Bureau/Scolaire/M1-S2/Projet kaggle/sentiment.csv', index=False)\n",
    "\n",
    "#df_sentiment = pd.read_csv('/home/cmisid/Bureau/Scolaire/M1-S2/Projet kaggle/sentiment.csv')\n",
    "#df_txt['sentiment_polarity'] = list(df_sentiment['sentiment_polarity'])\n",
    "#df_txt['sentiment_subjectivity'] = list(df_sentiment['sentiment_subjectivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat['sentiment_polarity'] = list(df_txt['sentiment_polarity'])\n",
    "df_feat['sentiment_subjectivity'] = list(df_txt['sentiment_subjectivity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du modèle\n",
    "glove_input_file = 'glove.6B.50d.txt'\n",
    "word2vec_output_file = 'glove.6B.50d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement du modèle\n",
    "filename = 'glove.6B.50d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenisation du corpus afin de pouvoir appliquer le modèle à nos différents mots\n",
    "sentences = [word_tokenize(s) for s in df_txt_clean['corpus']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'un dictionnaire de mots afin d'optimiser les temps de traitement\n",
    "dict_txt = {}\n",
    "for i in tqdm_notebook(range(len(df_txt_clean))):\n",
    "    dict_txt[str(i)] = sentences[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_vect\n",
    "# input : un mot, le modèle\n",
    "# output : retourne le vecteur du mot ou un vecteur de 0 si le mot n'est pas reconnu dans le modèle\n",
    "def get_vect(word, model):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except KeyError:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# sum_vectors\n",
    "# input : une phrase, le modèle\n",
    "# output : retourne la somme de vecteur pour l'ensemble de mots de la phrase\n",
    "def sum_vectors(phrase, model):\n",
    "    return sum(get_vect(w, model) for w in phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'un dictionnaire avec les différents vecteurs\n",
    "dict_vec = {}\n",
    "for i in tqdm_notebook(range(len(dict_txt))):\n",
    "    dict_vec[str(i)] = sum_vectors(dict_txt[str(i)], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sauvegarde et chargement du w2v  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde du dictionnaire\n",
    "with open('dict_w2v.csv', 'w', newline=\"\") as csv_file:  \n",
    "    writer = csv.writer(csv_file)\n",
    "    for value in tqdm_notebook(dict_vec.values()):\n",
    "        writer.writerow(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertission du dictionnaire en data frame (impossibilité de le faire directement avec panda.DataFrame.from_dic)\n",
    "w2v = pd.read_csv('dict_w2v.csv', sep = \",\", header = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### w2v kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applicatino de kmeans sur les vecteurs\n",
    "kmeans = KMeans(100).fit(w2v)\n",
    "df_feat['kmeans_w2v'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features avec graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_c = nx.degree_centrality(g)\n",
    "df_deg = pd.DataFrame.from_dict(degree_c, orient='index')\n",
    "df_deg = df_deg.reset_index()\n",
    "df_deg = df_deg[df_deg['index'].isin(list(df['name']))]\n",
    "df_feat['degree_c'] = list(df_deg[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closeness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeness_c = nx.closeness_centrality(g)\n",
    "df_deg = pd.DataFrame.from_dict(closeness_c, orient='index')\n",
    "df_deg = df_deg.reset_index()\n",
    "df_deg = df_deg[df_deg['index'].isin(list(df['name']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat['closeness_c'] = list(df_deg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat.to_csv(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Granularité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'link_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/home/salem/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'link_id'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-29c2b75f2d13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mlink_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'link_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlink_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlist_path_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortest_path_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/salem/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/salem/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'link_id'"
     ]
    }
   ],
   "source": [
    "# Ici on calcule le \"rang\" du commentaire, i.e. 1 si c'est le premier commentaire, 2 si c'est\n",
    "# un commentaire de commentaire et ainsi de suite. On ajoute 1 si on a pas le lien jusqu'au \n",
    "# noeud principal T3.\n",
    "# Pour cela on utilise la fonction shortest_path_length de network qui calcule les plus petits\n",
    "# chemins entre un noeud et tous les autres noeuds du réseau. On utilise ensuite le max pour \n",
    "# trouver le rang.\n",
    "list_path_length = []\n",
    "for i in df.index :\n",
    "    name = df['name'][i]\n",
    "    link_id = df['link_id'][i]\n",
    "    if nx.has_path(g, name, link_id):\n",
    "        list_path_length.append(max(nx.shortest_path_length(g, name).values()))\n",
    "    else :\n",
    "        list_path_length.append(max(nx.shortest_path_length(g, name).values())+1)\n",
    "df_feat['path_length'] = list_path_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laps de temps entre un commentaire et son parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On utilise un df intermédiaire où on enlève les colonnes innutiles\n",
    "# On met la colonne 'name' en index, on crée un dictionnaire afin d'optimiser le temps de calcul\n",
    "df_inter = df.drop(['parent_id','ups','link_id','body','id'], axis = 'columns')\n",
    "df_inter = df_inter.set_index('name')\n",
    "dict_name_epoch = df_inter.to_dict()['created_utc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_time_diff = []\n",
    "\n",
    "# On parcourt la colonne 'name' du dataframe principal. On sélectionne le commentaire parent\n",
    "# puis on teste si ce parent existe et si ce n'est pas un noeud 't3' car on a pas l'heure des \n",
    "# publications. Lorsqu'on a pas les informations on remplit avec np.NaN, sinon on remplit la\n",
    "# différence entre le commentaire et son parent\n",
    "\n",
    "for i in df['name']:\n",
    "    parent_id = list(g.successors(i))\n",
    "    if parent_id == []: \n",
    "        list_time_diff.append(np.NaN)\n",
    "    elif 't3_' in parent_id[0]: #on exclut les noeuds avec 't3_' car pas dans le dataset\n",
    "        list_time_diff.append(np.NaN)\n",
    "    elif parent_id[0] in dict_name_epoch.keys():\n",
    "        parent_time = dict_name_epoch[parent_id[0]]\n",
    "        child_time = dict_name_epoch[i]\n",
    "        list_time_diff.append((child_time - parent_time)/60)\n",
    "    else : \n",
    "        list_time_diff.append(np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat['time_diff'] = list_time_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperçu du data frame avec les features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_feat.to_csv('/home/cmisid/Bureau/Scolaire/M1-S2/Projet kaggle/features.csv', index=False)\n",
    "#df_feat = pd.read_csv('/home/cmisid/Bureau/Scolaire/M1-S2/Projet kaggle/features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTRAINEMENT DU MODELE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATION DES DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# séparation du jeu de validation\n",
    "df_val = df_feat[df_feat['ups'].isnull() == True]\n",
    "df_s = df_feat[df_feat['ups'].isnull() == False]\n",
    "\n",
    "# séparation des jeux d'entrainement et de tests\n",
    "y = df_s['ups']\n",
    "X = df_s.drop('ups', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELE LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference = lgb_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETRISATION DU MODELE & ENTRAINEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth' : 7,\n",
    "    'objective': 'regression_l1',\n",
    "    'metric': 'l1',\n",
    "    'num_leaves': 100,\n",
    "    'learning_rate': 0.01\n",
    "}\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round = 500,\n",
    "                valid_sets = lgb_eval,\n",
    "                early_stopping_rounds = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANCE DES FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(gbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTION SUR LE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbm.predict(X_test, num_iteration = gbm.best_iteration)\n",
    "print('The mae of prediction is :', mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTION SUR LA VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_y = df_val['ups']\n",
    "df_val_x = df_val.drop('ups',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = gbm.predict(df_val_x, num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du dataframe et exportation en .csv pour la soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_val = df[df['ups'].isnull() == True]\n",
    "id_val = list(id_val['id'])\n",
    "df_sub = pd.DataFrame({'id':id_val,'predicted':y_val})\n",
    "df_sub.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features innutilisées pour la meilleure soumission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du data frame en convertissant le created_utc\n",
    "df_time = pd.DataFrame(columns=['year', 'month', 'day', 'hour'])\n",
    "for time in tqdm_notebook(df['created_utc']):\n",
    "    this_time = datetime.utcfromtimestamp(time)\n",
    "    year = this_time.strftime(\"%Y\")\n",
    "    month = this_time.strftime(\"%m\")\n",
    "    day = this_time.strftime(\"%d\")\n",
    "    hour = this_time.strftime(\"%H\")\n",
    "    df_time.loc[len(df_time)] = [year, month, day, hour]\n",
    "\n",
    "# création de la feature hour plus rapide\n",
    "df_feat['hour'] = df['created_utc'].apply(lambda x: datetime.utcfromtimestamp(x).strftime(\"%H\"))\n",
    "df_feat['hour'] = pd.to_numeric(df_feat['hour'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF & K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du tf idf\n",
    "tfidf = TfidfVectorizer(max_features = 50, analyzer = 'word', ngram_range=(1,2), max_df = 0.80, min_df = 0.04)\n",
    "df_txt_tfidf = tfidf.fit_transform(df_txt_clean['corpus'])\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "dense = df_txt_tfidf.todense()\n",
    "denselist = dense.tolist()\n",
    "tf_idf = pd.DataFrame(denselist, columns=feature_names)\n",
    "\n",
    "# Appliquer un k-means sur notre tf-idf\n",
    "kmeans = KMeans(30).fit(tf_idf)\n",
    "df_feat['tfidf_kmeans'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prestige degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = len(g_nodes)\n",
    "degree_prestige = dict((v,len(g.in_edges(v))/(n_nodes-1)) for v in g.nodes())\n",
    "df_prestige = pd.DataFrame.from_dict(degree_prestige, orient='index')\n",
    "df_prestige = df_prestige.reset_index()\n",
    "df_prestige = df_prestige[df_prestige['index'].isin(list(df['name']))]\n",
    "df_feat['degree_prestige'] = list(df_prestige[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upvotes du commentaire parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un dictionnaire pour interroger les données plus rapidement\n",
    "# (évite de créer un dataframe avec .iloc() pour une recherche par condition)\n",
    "# Le dictionnaire a la structure suivante : \n",
    "# {'name1' : ups1,\n",
    "# 'name2' : ups2,\n",
    "# ...\n",
    "# }\n",
    "\n",
    "df_inter = df.drop(['parent_id','created_utc','link_id','body'], axis = 'columns')\n",
    "df_inter = df_inter.set_index('name')\n",
    "dict_name_ups = df_inter.to_dict()['ups']\n",
    "a = []\n",
    "for i in df['name']:\n",
    "    parent_id = list(g.successors(i))\n",
    "    if parent_id == []: \n",
    "        a.append(np.NaN)\n",
    "    elif 't3_' in parent_id[0]: #on exclut les noeuds avec 't3_' car pas dans le dataset\n",
    "        a.append(np.NaN)\n",
    "    elif parent_id[0] in dict_name_ups.keys():\n",
    "        var = dict_name_ups[parent_id[0]]\n",
    "        a.append(var)\n",
    "    else : \n",
    "        a.append(np.NaN)\n",
    "\n",
    "df_feat['parent_ups'] = a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average neighbor degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_av_deg = nx.average_neighbor_degree(g)\n",
    "df_av_ndeg = pd.DataFrame.from_dict(dict_av_deg, orient='index')\n",
    "df_av_ndeg = df_av_ndeg.reset_index()\n",
    "df_av_ndeg = df_av_ndeg[df_av_ndeg['index'].isin(list(df['name']))]\n",
    "df_feat['average_ndeg'] = list(df_av_ndeg[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moyenne des upvotes par heure (code pas optimisé)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created_utc'] = hour\n",
    "# on sépare les commentaires et les commentaires avec un 't3_' comme parent_id\n",
    "mean_per_hour = []\n",
    "mean_per_hourt3 = []\n",
    "df_alt_t3 = df[df['parent_id'].str.contains('t3_')]\n",
    "df_alt_t1 = df[df['parent_id'].str.contains('t1_')]\n",
    "for i in range (24):\n",
    "    df_alt = df_alt_t1[df_alt_t1['created_utc']==i]\n",
    "    df_t3 = df_alt_t3[df_alt_t3['created_utc']==i]\n",
    "    mean_per_hourt3.append(np.mean(df_t3['ups']))\n",
    "    mean_per_hour.append(np.mean(df_alt['ups']))\n",
    "tab_mean = []\n",
    "for i in range(len(df)):\n",
    "    for k in range(24):\n",
    "        if df['created_utc'][i] == k :\n",
    "            if 't3_' in df['parent_id'][i] :\n",
    "                tab_mean.append(mean_per_hourt3[k])\n",
    "            else :\n",
    "                tab_mean.append(mean_per_hour[k])\n",
    "df_feat['mean_per_hour'] = tab_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation d'une GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = lgb.LGBMRegressor(boosting_type= 'gbdt', \n",
    "          objective = 'regression_l1', \n",
    "          max_depth = 5,\n",
    "    metric = 'l1',\n",
    "    num_leaves = 31,\n",
    "    learning_rate = 0.05\n",
    "          )\n",
    "gridParams = {\n",
    "    'learning_rate': [0.001],\n",
    "    'n_estimators': [100],\n",
    "    'num_leaves': [31,100,250], # large num_leaves helps improve accuracy but might lead to over-fitting\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'bagging_fraction' : [0.2,0.5,0.8],\n",
    "    'min_data_in_leaf' : [200,600,1000]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(mdl, gridParams)\n",
    "# Run the grid\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
