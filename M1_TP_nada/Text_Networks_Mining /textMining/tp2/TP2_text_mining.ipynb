{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Université Paul Sabatier\n",
    "## \tEMMAB2C1 : Algorithmes de classification, data mining et text mining\n",
    "## Enseignant : José G. Moreno\n",
    "## Étudiant : \n",
    "## 02/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TP 2. Méthodes non-supervisées\n",
    " \n",
    "Dans ce TP nous allons travailler avec les résumés automatiques de la Wikipédia en français générés par Yahoo et disponibles pour téléchargement dans le lien :\n",
    "\n",
    "\n",
    "https://filesender.renater.fr/?s=download&token=eaf25baf-ef17-3da6-ae2a-0b4fbc4aac28\n",
    "\n",
    "\n",
    "Dans cette collection, chaque ligne correspond à une page Wikipédia avec son résumé (abstract), titre (title) et son adresse (url).\n",
    "\n",
    "Si nécessaire, installez les packages pandas, nltk et sckitlearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --user scikit-learn\n",
    "! pip3 install --user nltk\n",
    "! pip3 install --user pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Méthode naive pour le traitement d'une collection de grande taille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtfi = TfidfVectorizer()\n",
    "m = vtfi.fit_transform(pd.read_csv(\"frwiki-20170201-abstract.csv\")['abstract'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Méthode par chunck pour le traitement d'une collection de grande taille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtfi = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Décompressez et lisez le document csv téléchargé. Comme le fichier csv est volumineux, il est préférable de le lire et le processer par chunk (par bloque). Cette fonctionalité existe sur pandas (paramètre chunksize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               abstract             title  \\\n",
      "2000  1940 est une année bissextile commençant un lu...  Wikipédia : 1940   \n",
      "2001   1949 est une année commune commençant un samedi.  Wikipédia : 1949   \n",
      "2002                                   == Événements ==  Wikipédia : 1830   \n",
      "2003                                   == Événements ==  Wikipédia : 1836   \n",
      "2004                                   == Événements ==  Wikipédia : 1868   \n",
      "\n",
      "                                     url  \n",
      "2000  https://fr.wikipedia.org/wiki/1940  \n",
      "2001  https://fr.wikipedia.org/wiki/1949  \n",
      "2002  https://fr.wikipedia.org/wiki/1830  \n",
      "2003  https://fr.wikipedia.org/wiki/1836  \n",
      "2004  https://fr.wikipedia.org/wiki/1868  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "chunksize = 10 ** 3\n",
    "for i,chunk in enumerate(pd.read_csv(\"frwiki-20170201-abstract.csv\", chunksize=chunksize)):\n",
    "    if i == 2:\n",
    "        print(chunk.head())\n",
    "        vtfi.fit(chunk['abstract'].values)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vtfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               abstract  \\\n",
      "4000  thumb|Un pare-feu, représenté par un mur de br...   \n",
      "4001                    | image           = LogoISO.gif   \n",
      "4002  Un système de fichiers (abrégé « FS » pour , p...   \n",
      "4003                             | année_pop          =   \n",
      "4004                                  == Jeudi  2002 ==   \n",
      "\n",
      "                                                  title  \\\n",
      "4000                Wikipédia : Pare-feu (informatique)   \n",
      "4001  Wikipédia : Organisation internationale de nor...   \n",
      "4002                    Wikipédia : Système de fichiers   \n",
      "4003                                 Wikipédia : Toulon   \n",
      "4004                              Wikipédia : Août 2002   \n",
      "\n",
      "                                                    url  \n",
      "4000  https://fr.wikipedia.org/wiki/Pare-feu_(inform...  \n",
      "4001  https://fr.wikipedia.org/wiki/Organisation_int...  \n",
      "4002  https://fr.wikipedia.org/wiki/Syst%C3%A8me_de_...  \n",
      "4003               https://fr.wikipedia.org/wiki/Toulon  \n",
      "4004       https://fr.wikipedia.org/wiki/Ao%C3%BBt_2002  \n",
      "  (0, 3327)\t0.3542802479638958\n",
      "  (0, 3231)\t0.3142086663011592\n",
      "  (0, 2914)\t0.3960464799712189\n",
      "  (0, 2645)\t0.4792538071239405\n",
      "  (0, 2612)\t0.2754766435938433\n",
      "  (0, 2441)\t0.24602029249396168\n",
      "  (0, 2233)\t0.4792538071239405\n",
      "  (0, 1998)\t0.11400114656540707\n",
      "  (0, 1219)\t0.10063575625959453\n",
      "  (1, 1808)\t1.0\n",
      "  (2, 3328)\t0.19542197177056744\n",
      "  (2, 3327)\t0.2527267287643871\n",
      "  (2, 3170)\t0.23277999465693736\n",
      "  (2, 3143)\t0.09468297772701674\n",
      "  (2, 2767)\t0.12024008711492708\n",
      "  (2, 2612)\t0.19651197431012266\n",
      "  (2, 2450)\t0.1449122341622551\n",
      "  (2, 2422)\t0.07972808583836714\n",
      "  (2, 2381)\t0.11760514063174779\n",
      "  (2, 2126)\t0.17093846978262137\n",
      "  (2, 2118)\t0.1613329605886055\n",
      "  (2, 2053)\t0.1449122341622551\n",
      "  (2, 2007)\t0.16099296156679924\n",
      "  (2, 1998)\t0.040661505986358114\n",
      "  (2, 1844)\t0.1305528160086622\n",
      "  :\t:\n",
      "  (997, 1219)\t0.14061952490686735\n",
      "  (997, 1043)\t0.36552133483612853\n",
      "  (998, 3358)\t0.2370974602832254\n",
      "  (998, 3352)\t0.25121386790581746\n",
      "  (998, 3143)\t0.13914759556401757\n",
      "  (998, 3063)\t0.15933417651126158\n",
      "  (998, 2612)\t0.1443985465034081\n",
      "  (998, 2520)\t0.21931288995905845\n",
      "  (998, 2450)\t0.21296530205908445\n",
      "  (998, 2381)\t0.17283436725085624\n",
      "  (998, 2360)\t0.2370974602832254\n",
      "  (998, 2204)\t0.25121386790581746\n",
      "  (998, 2007)\t0.3548966885474015\n",
      "  (998, 1998)\t0.11951357943824374\n",
      "  (998, 1692)\t0.2270817096816765\n",
      "  (998, 1441)\t0.25121386790581746\n",
      "  (998, 1439)\t0.1860464293528821\n",
      "  (998, 1219)\t0.15825287480540667\n",
      "  (998, 1213)\t0.12457005080257437\n",
      "  (998, 1011)\t0.21931288995905845\n",
      "  (998, 762)\t0.25121386790581746\n",
      "  (998, 34)\t0.36212864822465085\n",
      "  (999, 1973)\t0.2367574792784106\n",
      "  (999, 1383)\t0.8050479777097356\n",
      "  (999, 1043)\t0.5439151124864964\n"
     ]
    }
   ],
   "source": [
    "for i,chunk in enumerate(pd.read_csv(\"frwiki-20170201-abstract.csv\", chunksize=chunksize)):\n",
    "    if i == 4:\n",
    "        print(chunk.head())\n",
    "        print(vtfi.transform(chunk['abstract'].values))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Représentez chaque ligne de la colonne \"abstract\" comme un document et calculez les matrices document-terme pour les fréquences (vecteur TF) et la version normalisé (vecteur TF-IDF). Il est probable que le calcule est possible à faire uniquement si vous le faite par chunk, donc utilisez le code ci-dessus pour le faire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** Réalisez le clustering de ces documents à l’aide des 3 méthodes vues en cours (k-means, LDA et NMF). Nous choisirons dans un premier temps k=10 (10 groupes) pour chaque méthode. Interprétez les résultats.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)** Imprimez les 10 mots plus représentatives pour chaque groupe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)** Comparaison de modèles: Exécutez les algorithmes plusieurs fois avec des valeurs de k égales à 20, 50 et 100 (le nombre de groupes). Imprimez les mot plus représentatives et interprétez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(6)** Notez que chaque document est composé, en moyenne, par 24 mots. Calculez les vecteurs TF et TF-IDF pour les documents correspond aux lignes 1734430 et 1302898. Ont-ils de mots en commun ? Comment peut-on améliorer la représentation pour trouver des relations entre documents qui ne partagent pas de mots (ou très peu) ? Appliquez la méthode pertinent après d'avoir fini votre analyse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
