{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input data\n",
    "X=np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1]])\n",
    "#output data\n",
    "Y=np.array([[0],[1],[1],[0]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    def __init__(self, inputSize, hiddenLayers, outputSize):\n",
    "        #synapses\n",
    "        self._syn0=np.random.random((inputSize,hiddenLayers))\n",
    "        self._syn1=np.random.random((hiddenLayers,outputSize))\n",
    "    \n",
    "    def nonlin(self, x, deriv=False):\n",
    "        if deriv:\n",
    "            return x*(1-x)\n",
    "        return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "    def run(self, X, Y, n_iter=60000):\n",
    "        for i in range(n_iter):\n",
    "            l0 = X\n",
    "            l1 = nonlin(np.dot(l0,self._syn0))\n",
    "            l2 = nonlin(np.dot(l1,self._syn1))\n",
    "            \n",
    "            l2_error = Y-l2\n",
    "            if (i+1) % 10000==0 or i ==0:\n",
    "                print(\"Iter=\",i+1, \"Error=\",np.mean(np.abs(l2_error)))\n",
    "            \n",
    "            l2_delta = l2_error * nonlin(l2, deriv=True)\n",
    "            l1_error = l2_delta.dot(self._syn1.T)\n",
    "            l1_delta = l1_error * nonlin(l1, deriv=True)\n",
    "            \n",
    "            #update\n",
    "            self._syn1 += l1.T.dot(l2_delta)\n",
    "            self._syn0 += l0.T.dot(l1_delta)\n",
    "        print(\"----\")\n",
    "        print(X)\n",
    "        print(Y)\n",
    "        print(l2)\n",
    "        print(np.argmax(l2, axis=1))\n",
    "    \n",
    "    def print_syns(self):\n",
    "        print(self._syn0)\n",
    "        print(self._syn1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter= 1 Error= 0.496566336035\n",
      "Iter= 10000 Error= 0.0124170290708\n",
      "Iter= 20000 Error= 0.00854169596933\n",
      "Iter= 30000 Error= 0.00689498561136\n",
      "Iter= 40000 Error= 0.005931564767\n",
      "Iter= 50000 Error= 0.00528147275821\n",
      "Iter= 60000 Error= 0.00480530519781\n",
      "----\n",
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "[[ 0.00138257]\n",
      " [ 0.99451964]\n",
      " [ 0.99447956]\n",
      " [ 0.00683785]]\n",
      "[0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "snn = SimpleNN(3,4,1)\n",
    "snn.run(X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.17462703  3.14277846  0.96706835  5.98528201]\n",
      " [ 6.63360455  4.25357779  4.51013659 -4.28574902]\n",
      " [-2.8765724  -5.62249783 -3.61743333  2.29305974]]\n",
      "[[ 13.24941049]\n",
      " [ -7.47081579]\n",
      " [ -6.70372468]\n",
      " [ -6.70078631]]\n"
     ]
    }
   ],
   "source": [
    "snn.print_syns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "workpath=\"/data/jmoreno/Datasets/signalmedia/\"\n",
    "\n",
    "f=gzip.open(workpath+'signalmedia-1k.jsonl.gz','rb')\n",
    "\n",
    "\n",
    "docs=[json.loads(s.decode('utf-8')) for s in f.readlines()]\n",
    "titres=[x['title'] for x in docs]\n",
    "content=[x['content'] for x in docs]\n",
    "print(len(titres))\n",
    "\n",
    "def sentense2cleanTokens(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = \"\".join([x if x.isalpha() else \" \" for x in sent])\n",
    "    sent = \" \".join(sent.split())\n",
    "    return sent\n",
    "\n",
    "\n",
    "cleantitres=[sentense2cleanTokens(x) for x in titres]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vectorizer = CountVectorizer(min_df=2, stop_words='english')\n",
    "tfsk = tf_vectorizer.fit_transform(cleantitres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'years': 1066,\n",
       " 'hughes': 460,\n",
       " 'sister': 886,\n",
       " 'jailed': 496,\n",
       " 'seven': 873,\n",
       " 'round': 828,\n",
       " 'sitting': 888,\n",
       " 'jennifer': 499,\n",
       " 'premiere': 728,\n",
       " 'showing': 882,\n",
       " 'professional': 746,\n",
       " 'dubai': 289,\n",
       " 'photo': 696,\n",
       " 'nuclear': 647,\n",
       " 'pro': 741,\n",
       " 'car': 132,\n",
       " 'official': 656,\n",
       " 'unique': 1007,\n",
       " 'ruled': 834,\n",
       " 'flowers': 365,\n",
       " 'closing': 177,\n",
       " 'great': 419,\n",
       " 'suspicious': 945,\n",
       " 'awarded': 66,\n",
       " 'loan': 549,\n",
       " 'jobs': 502,\n",
       " 'deal': 244,\n",
       " 'frankfurt': 378,\n",
       " 'steps': 919,\n",
       " 'conference': 197,\n",
       " 'recycling': 786,\n",
       " 'brian': 112,\n",
       " 'opener': 662,\n",
       " 'predictions': 727,\n",
       " 'goes': 410,\n",
       " 'parents': 676,\n",
       " 'foreign': 372,\n",
       " 'schedule': 847,\n",
       " 'boost': 102,\n",
       " 'daniel': 234,\n",
       " 'outside': 669,\n",
       " 'hiring': 444,\n",
       " 'live': 545,\n",
       " 'surprise': 941,\n",
       " 'aussie': 59,\n",
       " 'art': 51,\n",
       " 'plc': 713,\n",
       " 'target': 956,\n",
       " 'products': 745,\n",
       " 'brad': 107,\n",
       " 'revealed': 810,\n",
       " 'cup': 227,\n",
       " 'welcomes': 1044,\n",
       " 'dogs': 273,\n",
       " 'paul': 686,\n",
       " 'blatter': 91,\n",
       " 'carry': 135,\n",
       " 'registration': 792,\n",
       " 'begin': 82,\n",
       " 'ford': 370,\n",
       " 'incident': 470,\n",
       " 'plays': 712,\n",
       " 'party': 681,\n",
       " 'munich': 622,\n",
       " 'electronic': 302,\n",
       " 'vows': 1027,\n",
       " 'super': 938,\n",
       " 'bad': 71,\n",
       " 'waves': 1038,\n",
       " 'refugees': 790,\n",
       " 'private': 739,\n",
       " 'term': 962,\n",
       " 'county': 214,\n",
       " 'issue': 493,\n",
       " 'uber': 1002,\n",
       " 'tigers': 973,\n",
       " 'fantasy': 341,\n",
       " 'apple': 45,\n",
       " 'release': 795,\n",
       " 'treatment': 987,\n",
       " 'bodied': 97,\n",
       " 'election': 300,\n",
       " 'mental': 595,\n",
       " 'app': 44,\n",
       " 'channel': 152,\n",
       " 'british': 115,\n",
       " 'shoes': 879,\n",
       " 'streaming': 927,\n",
       " 'fake': 334,\n",
       " 'stadiums': 908,\n",
       " 'stocks': 922,\n",
       " 'rises': 817,\n",
       " 'come': 187,\n",
       " 'mom': 608,\n",
       " 'table': 952,\n",
       " 'better': 86,\n",
       " 'smaller': 890,\n",
       " 'end': 306,\n",
       " 'georgia': 395,\n",
       " 'draft': 281,\n",
       " 'break': 109,\n",
       " 'create': 219,\n",
       " 'bn': 95,\n",
       " 'reasons': 781,\n",
       " 'council': 211,\n",
       " 'couple': 215,\n",
       " 'rugby': 832,\n",
       " 'completely': 195,\n",
       " 'investors': 489,\n",
       " 'accused': 2,\n",
       " 'sector': 859,\n",
       " 'tour': 981,\n",
       " 'vegas': 1019,\n",
       " 'september': 866,\n",
       " 'closed': 174,\n",
       " 'travel': 986,\n",
       " 'wall': 1030,\n",
       " 'downtown': 279,\n",
       " 'getting': 399,\n",
       " 'talks': 955,\n",
       " 'organic': 667,\n",
       " 'cases': 138,\n",
       " 'german': 396,\n",
       " 'economic': 296,\n",
       " 'given': 404,\n",
       " 'ago': 15,\n",
       " 'worth': 1063,\n",
       " 'dollar': 274,\n",
       " 'construction': 203,\n",
       " 'youth': 1070,\n",
       " 'corporation': 210,\n",
       " 'beautiful': 80,\n",
       " 'davis': 240,\n",
       " 'host': 455,\n",
       " 'power': 724,\n",
       " 'class': 169,\n",
       " 'migrant': 600,\n",
       " 'months': 615,\n",
       " 'freedom': 380,\n",
       " 'stands': 911,\n",
       " 'lovers': 561,\n",
       " 'doesn': 272,\n",
       " 'cast': 140,\n",
       " 'performing': 692,\n",
       " 'eyes': 328,\n",
       " 'jerry': 500,\n",
       " 'markets': 579,\n",
       " 'prices': 736,\n",
       " 'wary': 1034,\n",
       " 'claims': 167,\n",
       " 'derby': 255,\n",
       " 'despite': 257,\n",
       " 'ministers': 604,\n",
       " 'blogging': 93,\n",
       " 'vs': 1028,\n",
       " 'long': 552,\n",
       " 'huge': 459,\n",
       " 'expands': 324,\n",
       " 'london': 551,\n",
       " 'announced': 37,\n",
       " 'launch': 526,\n",
       " 'deals': 245,\n",
       " 'campaign': 128,\n",
       " 'team': 959,\n",
       " 'white': 1046,\n",
       " 'refugee': 789,\n",
       " 'conspiracy': 202,\n",
       " 'recap': 782,\n",
       " 'university': 1009,\n",
       " 'blue': 94,\n",
       " 'wars': 1033,\n",
       " 'worcester': 1059,\n",
       " 'support': 940,\n",
       " 'spring': 905,\n",
       " 'news': 637,\n",
       " 'quarter': 762,\n",
       " 'cowboys': 217,\n",
       " 'central': 146,\n",
       " 'men': 594,\n",
       " 'marriage': 580,\n",
       " 'role': 826,\n",
       " 'tuesday': 997,\n",
       " 'fans': 340,\n",
       " 'according': 1,\n",
       " 'cities': 165,\n",
       " 'football': 368,\n",
       " 'project': 751,\n",
       " 'africa': 12,\n",
       " 'powers': 725,\n",
       " 'refinery': 788,\n",
       " 'route': 830,\n",
       " 'americans': 28,\n",
       " 'daughter': 238,\n",
       " 'wsj': 1064,\n",
       " 'mad': 564,\n",
       " 'hire': 443,\n",
       " 'kong': 516,\n",
       " 'nasa': 629,\n",
       " 'colorado': 185,\n",
       " 'regulations': 793,\n",
       " 'weekend': 1043,\n",
       " 'daily': 231,\n",
       " 'scam': 846,\n",
       " 'agrees': 16,\n",
       " 'china': 161,\n",
       " 'affected': 10,\n",
       " 'forecast': 371,\n",
       " 'notebook': 646,\n",
       " 'help': 438,\n",
       " 'testing': 964,\n",
       " 'trends': 989,\n",
       " 'fox': 376,\n",
       " 'drops': 288,\n",
       " 'employ': 305,\n",
       " 'nfl': 638,\n",
       " 'selling': 862,\n",
       " 'ride': 814,\n",
       " 'student': 931,\n",
       " 'networks': 635,\n",
       " 'san': 842,\n",
       " 'hits': 447,\n",
       " 'santa': 843,\n",
       " 'karen': 510,\n",
       " 'matrix': 583,\n",
       " 'gang': 391,\n",
       " 'watch': 1036,\n",
       " 'style': 935,\n",
       " 'looks': 555,\n",
       " 'west': 1045,\n",
       " 'wide': 1047,\n",
       " 'schools': 849,\n",
       " 'added': 7,\n",
       " 'celebrity': 144,\n",
       " 'hero': 439,\n",
       " 'position': 722,\n",
       " 'say': 844,\n",
       " 'music': 625,\n",
       " 'overnight': 670,\n",
       " 'study': 933,\n",
       " 'fifa': 351,\n",
       " 'alan': 21,\n",
       " 'govt': 417,\n",
       " 'interior': 481,\n",
       " 'majority': 567,\n",
       " 'ex': 317,\n",
       " 'continue': 205,\n",
       " 'canadian': 129,\n",
       " 'usa': 1014,\n",
       " 'bonds': 100,\n",
       " 'return': 806,\n",
       " 'stars': 914,\n",
       " 'anderson': 32,\n",
       " 'minister': 603,\n",
       " 'growing': 425,\n",
       " 'putting': 761,\n",
       " 'faces': 331,\n",
       " 'proposed': 754,\n",
       " 'sep': 863,\n",
       " 'continues': 206,\n",
       " 'film': 354,\n",
       " 'las': 523,\n",
       " 'exhibition': 321,\n",
       " 'pressure': 733,\n",
       " 'opening': 663,\n",
       " 'quarterly': 763,\n",
       " 'winner': 1051,\n",
       " 'park': 677,\n",
       " 'robert': 823,\n",
       " 'annie': 34,\n",
       " 'pulled': 759,\n",
       " 'financing': 358,\n",
       " 'held': 437,\n",
       " 'visit': 1024,\n",
       " 'names': 628,\n",
       " 'stop': 923,\n",
       " 'woman': 1055,\n",
       " 'murder': 623,\n",
       " 'urges': 1013,\n",
       " 'celebrates': 143,\n",
       " 'beta': 85,\n",
       " 'chicago': 157,\n",
       " 'fda': 344,\n",
       " 'women': 1056,\n",
       " 'offers': 653,\n",
       " 'cuba': 226,\n",
       " 'robredo': 824,\n",
       " 'fail': 332,\n",
       " 'illinois': 465,\n",
       " 'devops': 261,\n",
       " 'records': 785,\n",
       " 'river': 821,\n",
       " 'peter': 694,\n",
       " 'online': 660,\n",
       " 'improve': 469,\n",
       " 'secrets': 858,\n",
       " 'landmark': 519,\n",
       " 'sea': 853,\n",
       " 'homes': 453,\n",
       " 'fair': 333,\n",
       " 'stock': 921,\n",
       " 'ahead': 17,\n",
       " 'pictures': 698,\n",
       " 'sheriff': 878,\n",
       " 'dividend': 270,\n",
       " 'revenue': 812,\n",
       " 'assistant': 55,\n",
       " 'risky': 819,\n",
       " 'gaming': 390,\n",
       " 'smith': 891,\n",
       " 'right': 815,\n",
       " 'investigation': 486,\n",
       " 'empire': 304,\n",
       " 'receives': 783,\n",
       " 'program': 749,\n",
       " 'shot': 881,\n",
       " 'brought': 118,\n",
       " 'far': 342,\n",
       " 'mortgage': 617,\n",
       " 'food': 367,\n",
       " 'rd': 776,\n",
       " 'golf': 412,\n",
       " 'repairs': 798,\n",
       " 'know': 515,\n",
       " 'girls': 402,\n",
       " 'adam': 6,\n",
       " 'airport': 20,\n",
       " 'nike': 641,\n",
       " 'form': 373,\n",
       " 'arctic': 48,\n",
       " 'company': 193,\n",
       " 'auction': 57,\n",
       " 'baltimore': 72,\n",
       " 'ultimate': 1005,\n",
       " 'tips': 976,\n",
       " 'speed': 902,\n",
       " 'nearly': 633,\n",
       " 'david': 239,\n",
       " 'summit': 937,\n",
       " 'agenda': 14,\n",
       " 'regional': 791,\n",
       " 'time': 974,\n",
       " 'focus': 366,\n",
       " 'byron': 126,\n",
       " 'imaging': 466,\n",
       " 'russia': 836,\n",
       " 'african': 13,\n",
       " 'coalition': 181,\n",
       " 'maple': 574,\n",
       " 'officials': 657,\n",
       " 'exclusively': 319,\n",
       " 'rejects': 794,\n",
       " 'owners': 671,\n",
       " 'eu': 311,\n",
       " 'marcus': 575,\n",
       " 'staff': 909,\n",
       " 'set': 871,\n",
       " 'tottenham': 980,\n",
       " 'rapids': 770,\n",
       " 'thousands': 968,\n",
       " 'times': 975,\n",
       " 'america': 26,\n",
       " 'lower': 563,\n",
       " 'cuts': 229,\n",
       " 'award': 65,\n",
       " 'outcome': 668,\n",
       " 'offer': 652,\n",
       " 'dream': 282,\n",
       " 'croatia': 224,\n",
       " 'health': 433,\n",
       " 'record': 784,\n",
       " 'choice': 162,\n",
       " 'price': 735,\n",
       " 'll': 548,\n",
       " 'won': 1057,\n",
       " 'case': 137,\n",
       " 'dodgers': 271,\n",
       " 'boot': 103,\n",
       " 'old': 659,\n",
       " 'sepp': 864,\n",
       " 'tide': 972,\n",
       " 'guide': 427,\n",
       " 'trial': 991,\n",
       " 'monkeys': 613,\n",
       " 'mission': 605,\n",
       " 'report': 799,\n",
       " 'announce': 36,\n",
       " 'asian': 53,\n",
       " 'boys': 105,\n",
       " 'using': 1017,\n",
       " 'medical': 588,\n",
       " 'legal': 536,\n",
       " 'germany': 397,\n",
       " 'burgess': 122,\n",
       " 'letter': 539,\n",
       " 'change': 150,\n",
       " 'annual': 40,\n",
       " 'breakfast': 110,\n",
       " 'patients': 684,\n",
       " 'winners': 1052,\n",
       " 'development': 259,\n",
       " 'south': 900,\n",
       " 'nyse': 649,\n",
       " 'largest': 522,\n",
       " 'fed': 347,\n",
       " 'chart': 154,\n",
       " 'austin': 60,\n",
       " 'flood': 363,\n",
       " 'making': 569,\n",
       " 'golden': 411,\n",
       " 'law': 529,\n",
       " 'ram': 768,\n",
       " 'computer': 196,\n",
       " 'questions': 764,\n",
       " 'holiday': 450,\n",
       " 'liver': 546,\n",
       " 'fight': 353,\n",
       " 'mayor': 586,\n",
       " 'nigeria': 639,\n",
       " 'twitter': 999,\n",
       " 'really': 780,\n",
       " 'investor': 488,\n",
       " 'family': 338,\n",
       " 'loss': 558,\n",
       " 'plans': 706,\n",
       " 'anthony': 41,\n",
       " 'soccer': 892,\n",
       " 'advance': 9,\n",
       " 'issues': 494,\n",
       " 'manchester': 573,\n",
       " 'students': 932,\n",
       " 'moving': 620,\n",
       " 'station': 918,\n",
       " 'arrested': 49,\n",
       " 'moody': 616,\n",
       " 'generation': 394,\n",
       " 'international': 483,\n",
       " 'child': 159,\n",
       " 'shape': 875,\n",
       " 'festival': 349,\n",
       " 'fears': 346,\n",
       " 'oil': 658,\n",
       " 'risk': 818,\n",
       " 'resources': 804,\n",
       " 'durable': 290,\n",
       " 'cfo': 148,\n",
       " 'industry': 472,\n",
       " 'kim': 514,\n",
       " 'padres': 673,\n",
       " 'tax': 957,\n",
       " 'officer': 655,\n",
       " 'based': 76,\n",
       " 'decisions': 250,\n",
       " 'meeting': 590,\n",
       " 'wednesday': 1041,\n",
       " 'korea': 517,\n",
       " 'dies': 266,\n",
       " 'executive': 320,\n",
       " 'author': 62,\n",
       " 'important': 468,\n",
       " 'life': 541,\n",
       " 'awareness': 68,\n",
       " 'uk': 1003,\n",
       " 'board': 96,\n",
       " 'foundation': 374,\n",
       " 'jordan': 505,\n",
       " 'rat': 771,\n",
       " 'college': 182,\n",
       " 'prize': 740,\n",
       " 'fund': 385,\n",
       " 'anniversary': 35,\n",
       " 'exposure': 326,\n",
       " 'analysts': 31,\n",
       " 'takes': 953,\n",
       " 'los': 556,\n",
       " 'fall': 335,\n",
       " 'corbyn': 208,\n",
       " 'false': 337,\n",
       " 'goods': 414,\n",
       " 'innovative': 476,\n",
       " 'growth': 426,\n",
       " 'says': 845,\n",
       " 'human': 461,\n",
       " 'upset': 1012,\n",
       " 'madison': 565,\n",
       " 'late': 524,\n",
       " 'climate': 172,\n",
       " 'vote': 1026,\n",
       " 'shop': 880,\n",
       " 'kicks': 512,\n",
       " 'launches': 528,\n",
       " 'california': 127,\n",
       " 'like': 542,\n",
       " 'don': 275,\n",
       " 'meet': 589,\n",
       " 'lose': 557,\n",
       " 'self': 861,\n",
       " 'pay': 687,\n",
       " 'album': 22,\n",
       " 'clinton': 173,\n",
       " 'bring': 113,\n",
       " 'awards': 67,\n",
       " 'marijuana': 576,\n",
       " 'swot': 949,\n",
       " 'true': 994,\n",
       " 'near': 632,\n",
       " 'setting': 872,\n",
       " 'baby': 69,\n",
       " 'williams': 1048,\n",
       " 'serena': 867,\n",
       " 'pm': 715,\n",
       " 'celebrate': 142,\n",
       " 'body': 98,\n",
       " 'sale': 839,\n",
       " 'residents': 803,\n",
       " 'sex': 874,\n",
       " 'chelsea': 156,\n",
       " 'plunge': 714,\n",
       " 'wind': 1050,\n",
       " 'breast': 111,\n",
       " 'bank': 73,\n",
       " 'client': 171,\n",
       " 'members': 593,\n",
       " 'service': 869,\n",
       " 'federal': 348,\n",
       " 'new': 636,\n",
       " 'stuff': 934,\n",
       " 'goal': 408,\n",
       " 'brother': 117,\n",
       " 'children': 160,\n",
       " 'yen': 1067,\n",
       " 'line': 543,\n",
       " 'star': 913,\n",
       " 'confirms': 200,\n",
       " 'reveal': 809,\n",
       " 'valley': 1018,\n",
       " 'taking': 954,\n",
       " 'decision': 249,\n",
       " 'halt': 431,\n",
       " 'care': 133,\n",
       " 'monitor': 612,\n",
       " 'didn': 263,\n",
       " 'red': 787,\n",
       " 'big': 87,\n",
       " 'criminal': 222,\n",
       " 'professor': 747,\n",
       " 'fifth': 352,\n",
       " 'city': 166,\n",
       " 'field': 350,\n",
       " 'pole': 717,\n",
       " 'optical': 665,\n",
       " 'seattle': 856,\n",
       " 'afl': 11,\n",
       " 'ways': 1039,\n",
       " 'test': 963,\n",
       " 'good': 413,\n",
       " 'leads': 532,\n",
       " 'texas': 965,\n",
       " 'mega': 592,\n",
       " 'iran': 490,\n",
       " 'meets': 591,\n",
       " 'nd': 631,\n",
       " 'club': 179,\n",
       " 'companies': 192,\n",
       " 'present': 730,\n",
       " 'united': 1008,\n",
       " 'deputies': 254,\n",
       " 'darling': 236,\n",
       " 'partners': 679,\n",
       " 'application': 46,\n",
       " 'pope': 721,\n",
       " 'start': 915,\n",
       " 'process': 744,\n",
       " 'lead': 530,\n",
       " 'worker': 1061,\n",
       " 'relief': 797,\n",
       " 'mobile': 607,\n",
       " 'michael': 597,\n",
       " 'work': 1060,\n",
       " 'john': 503,\n",
       " 'transparent': 985,\n",
       " 'story': 925,\n",
       " 'rival': 820,\n",
       " 'victims': 1021,\n",
       " 'veterans': 1020,\n",
       " 'trees': 988,\n",
       " 'webinar': 1040,\n",
       " 'trescothick': 990,\n",
       " 'brazil': 108,\n",
       " 'swift': 946,\n",
       " 'dead': 242,\n",
       " 'plan': 704,\n",
       " 'type': 1000,\n",
       " 'stanford': 912,\n",
       " 'available': 64,\n",
       " 'fe': 345,\n",
       " 'data': 237,\n",
       " 'state': 917,\n",
       " 'probe': 742,\n",
       " 'somerset': 897,\n",
       " 'mercedes': 596,\n",
       " 'australia': 61,\n",
       " 'payment': 688,\n",
       " 'rate': 772,\n",
       " 'countries': 212,\n",
       " 'home': 451,\n",
       " 'single': 885,\n",
       " 'national': 630,\n",
       " 'solutions': 896,\n",
       " 'placement': 703,\n",
       " 'ashes': 52,\n",
       " 'trading': 984,\n",
       " 'screening': 852,\n",
       " 'photos': 697,\n",
       " 'intern': 482,\n",
       " 'wage': 1029,\n",
       " 'day': 241,\n",
       " 'facebook': 330,\n",
       " 'dreams': 283,\n",
       " 'cash': 139,\n",
       " 'progress': 750,\n",
       " 'strong': 930,\n",
       " 'paper': 675,\n",
       " 'evicted': 316,\n",
       " 'benefits': 83,\n",
       " 'ft': 384,\n",
       " 'rated': 773,\n",
       " 'row': 831,\n",
       " 'capitals': 131,\n",
       " 'jose': 506,\n",
       " 'releases': 796,\n",
       " 'game': 388,\n",
       " 'exchange': 318,\n",
       " 'easy': 295,\n",
       " 'rocket': 825,\n",
       " 'interviews': 484,\n",
       " 'japan': 497,\n",
       " 'head': 432,\n",
       " 'auto': 63,\n",
       " 'tom': 979,\n",
       " 'earns': 292,\n",
       " 'monday': 610,\n",
       " 'spirit': 904,\n",
       " 'injury': 474,\n",
       " 'love': 560,\n",
       " 'soul': 899,\n",
       " 'francis': 377,\n",
       " 'patrol': 685,\n",
       " 'fan': 339,\n",
       " 'dad': 230,\n",
       " 'labour': 518,\n",
       " 'wash': 1035,\n",
       " 'expo': 325,\n",
       " 'hunt': 462,\n",
       " 'rankings': 769,\n",
       " 'reach': 777,\n",
       " 'planned': 705,\n",
       " 'mars': 581,\n",
       " 'financial': 357,\n",
       " 'season': 855,\n",
       " 'possible': 723,\n",
       " 'garden': 392,\n",
       " 'opens': 664,\n",
       " 'ready': 778,\n",
       " 'water': 1037,\n",
       " 'buzz': 125,\n",
       " 'brace': 106,\n",
       " 'playing': 711,\n",
       " 'starts': 916,\n",
       " 'learning': 535,\n",
       " 'analysis': 30,\n",
       " 'taylor': 958,\n",
       " 'village': 1023,\n",
       " 'die': 264,\n",
       " 'strike': 929,\n",
       " 'partner': 678,\n",
       " 'space': 901,\n",
       " 'ticket': 971,\n",
       " 'fashion': 343,\n",
       " 'week': 1042,\n",
       " 'mps': 621,\n",
       " 'movers': 619,\n",
       " 'office': 654,\n",
       " 'actually': 5,\n",
       " 'research': 801,\n",
       " 'want': 1031,\n",
       " 'supplements': 939,\n",
       " 'contract': 207,\n",
       " 'promises': 752,\n",
       " 'emission': 303,\n",
       " 'poll': 720,\n",
       " 'coach': 180,\n",
       " 'amid': 29,\n",
       " 'industrial': 471,\n",
       " 'ali': 23,\n",
       " 'anti': 42,\n",
       " 'site': 887,\n",
       " 'dame': 232,\n",
       " 'clerk': 170,\n",
       " 'face': 329,\n",
       " 'giveaway': 403,\n",
       " 'episode': 309,\n",
       " 'roundup': 829,\n",
       " 'chase': 155,\n",
       " 'collins': 183,\n",
       " 'non': 643,\n",
       " 'lands': 520,\n",
       " 'property': 753,\n",
       " 'raises': 767,\n",
       " 'join': 504,\n",
       " 'problem': 743,\n",
       " 'debate': 247,\n",
       " 'jet': 501,\n",
       " 'president': 731,\n",
       " 'reveals': 811,\n",
       " 'com': 186,\n",
       " 'early': 291,\n",
       " 'warns': 1032,\n",
       " 'christmas': 163,\n",
       " 'free': 379,\n",
       " 'local': 550,\n",
       " 'jason': 498,\n",
       " 'driven': 285,\n",
       " 'innovation': 475,\n",
       " 'look': 553,\n",
       " 'thursday': 970,\n",
       " 'lets': 538,\n",
       " 'street': 928,\n",
       " 'insurance': 479,\n",
       " 'judge': 508,\n",
       " 'hall': 430,\n",
       " 'road': 822,\n",
       " 'try': 996,\n",
       " 'matter': 585,\n",
       " 'wood': 1058,\n",
       " 'exit': 323,\n",
       " 'public': 758,\n",
       " 'career': 134,\n",
       " 'demand': 252,\n",
       " 'sept': 865,\n",
       " 'million': 602,\n",
       " 'key': 511,\n",
       " 'investment': 487,\n",
       " 'play': 709,\n",
       " 'challenge': 149,\n",
       " 'exhibits': 322,\n",
       " 'store': 924,\n",
       " 'amazon': 25,\n",
       " 'zero': 1071,\n",
       " 'rise': 816,\n",
       " 'radio': 766,\n",
       " 'pastor': 682,\n",
       " 'changer': 151,\n",
       " 'usd': 1015,\n",
       " 'survivor': 942,\n",
       " 'flats': 360,\n",
       " 'updates': 1011,\n",
       " 'spies': 903,\n",
       " 'department': 253,\n",
       " 'night': 640,\n",
       " 'results': 805,\n",
       " 'eastern': 294,\n",
       " 'friendly': 383,\n",
       " 'falls': 336,\n",
       " 'strain': 926,\n",
       " 'stage': 910,\n",
       " 'series': 868,\n",
       " 'crash': 218,\n",
       " 'soon': 898,\n",
       " 'lives': 547,\n",
       " 'beer': 81,\n",
       " 'performance': 691,\n",
       " 'digital': 267,\n",
       " 'solid': 895,\n",
       " 'announces': 39,\n",
       " 'august': 58,\n",
       " 'peace': 689,\n",
       " 'birthday': 88,\n",
       " 'market': 577,\n",
       " 'update': 1010,\n",
       " 'marketing': 578,\n",
       " 'backyard': 70,\n",
       " 'aid': 18,\n",
       " 'pink': 700,\n",
       " 'trip': 993,\n",
       " 'hotel': 457,\n",
       " 'list': 544,\n",
       " 'announcement': 38,\n",
       " 'assembly': 54,\n",
       " 'share': 876,\n",
       " 'reporting': 800,\n",
       " 'forced': 369,\n",
       " 'bay': 79,\n",
       " 'drive': 284,\n",
       " 'global': 406,\n",
       " 'consumers': 204,\n",
       " 'major': 566,\n",
       " 'provider': 756,\n",
       " 'northern': 644,\n",
       " 'obama': 650,\n",
       " 'community': 191,\n",
       " 'cut': 228,\n",
       " 'dark': 235,\n",
       " 'wins': 1054,\n",
       " 'build': 120,\n",
       " 'run': 835,\n",
       " 'slams': 889,\n",
       " 'high': 440,\n",
       " 'technology': 961,\n",
       " 'petersburg': 695,\n",
       " 'theatre': 967,\n",
       " 'services': 870,\n",
       " 'murray': 624,\n",
       " 'york': 1068,\n",
       " 'looking': 554,\n",
       " 'swiss': 947,\n",
       " 'homeland': 452,\n",
       " 'desire': 256,\n",
       " 'death': 246,\n",
       " 'suspects': 943,\n",
       " 'instead': 478,\n",
       " 'sales': 840,\n",
       " 'court': 216,\n",
       " 'future': 386,\n",
       " 'europe': 312,\n",
       " 'provides': 757,\n",
       " 'mystery': 626,\n",
       " 'abstract': 0,\n",
       " 'business': 123,\n",
       " 'final': 355,\n",
       " 'manager': 572,\n",
       " 'discovered': 269,\n",
       " 'battle': 78,\n",
       " 'american': 27,\n",
       " 'baptist': 74,\n",
       " 'heart': 436,\n",
       " 'steve': 920,\n",
       " 'broadway': 116,\n",
       " 'introduces': 485,\n",
       " 'greek': 422,\n",
       " 'town': 982,\n",
       " 'confident': 199,\n",
       " 'destroy': 258,\n",
       " 'clash': 168,\n",
       " 'winning': 1053,\n",
       " 'firm': 359,\n",
       " 'government': 416,\n",
       " 'ukraine': 1004,\n",
       " 'flight': 361,\n",
       " 'point': 716,\n",
       " 'comes': 188,\n",
       " 'corp': 209,\n",
       " 'double': 278,\n",
       " 'platform': 708,\n",
       " 'need': 634,\n",
       " 'open': 661,\n",
       " 'pitch': 701,\n",
       " 'committee': 190,\n",
       " 'matt': 584,\n",
       " 'impact': 467,\n",
       " 'noise': 642,\n",
       " 'giant': 400,\n",
       " 'pierce': 699,\n",
       " 'group': 424,\n",
       " 'shows': 883,\n",
       " 'partnership': 680,\n",
       " 'orders': 666,\n",
       " 'closes': 176,\n",
       " 'uae': 1001,\n",
       " 'education': 298,\n",
       " 'green': 423,\n",
       " 'thrones': 969,\n",
       " 'social': 893,\n",
       " 'gun': 429,\n",
       " 'patel': 683,\n",
       " 'jail': 495,\n",
       " 'money': 611,\n",
       " 'gives': 405,\n",
       " 'confidence': 198,\n",
       " 'building': 121,\n",
       " 'founder': 375,\n",
       " 'plants': 707,\n",
       " 'cancer': 130,\n",
       " 'primer': 737,\n",
       " 'leader': 531,\n",
       " 'did': 262,\n",
       " 'provide': 755,\n",
       " 'said': 838,\n",
       " 'debut': 248,\n",
       " 'general': 393,\n",
       " 'sam': 841,\n",
       " 'finals': 356,\n",
       " 'policy': 719,\n",
       " 'win': 1049,\n",
       " 'games': 389,\n",
       " 'press': 732,\n",
       " 'today': 978,\n",
       " 'rates': 774,\n",
       " 'russian': 837,\n",
       " 'man': 571,\n",
       " 'driving': 286,\n",
       " 'latest': 525,\n",
       " 'search': 854,\n",
       " 'police': 718,\n",
       " 'arbor': 47,\n",
       " 'boston': 104,\n",
       " 'middle': 599,\n",
       " 'media': 587,\n",
       " 'competition': 194,\n",
       " 'cars': 136,\n",
       " 'hot': 456,\n",
       " 'florida': 364,\n",
       " 'volkswagen': 1025,\n",
       " 'th': 966,\n",
       " 'profile': 748,\n",
       " 'ceo': 147,\n",
       " 'tv': 998,\n",
       " 'device': 260,\n",
       " 'launched': 527,\n",
       " 'trade': 983,\n",
       " 'alzheimer': 24,\n",
       " 'crisis': 223,\n",
       " 'iiroc': 464,\n",
       " 'address': 8,\n",
       " 'highlights': 441,\n",
       " 'success': 936,\n",
       " 'collision': 184,\n",
       " 'landscape': 521,\n",
       " 'switch': 948,\n",
       " 'stable': 907,\n",
       " 'year': 1065,\n",
       " 'drop': 287,\n",
       " 'idea': 463,\n",
       " 'economy': 297,\n",
       " 'bar': 75,\n",
       " 'history': 445,\n",
       " 'hong': 454,\n",
       " 'east': 293,\n",
       " 'gm': 407,\n",
       " 'dot': 277,\n",
       " 'event': 315,\n",
       " 'journey': 507,\n",
       " 'roll': 827,\n",
       " 'just': 509,\n",
       " 'islands': 492,\n",
       " 'black': 90,\n",
       " 'league': 533,\n",
       " 'make': 568,\n",
       " 'gets': 398,\n",
       " 'google': 415,\n",
       " 'killed': 513,\n",
       " 'coming': 189,\n",
       " 'dose': 276,\n",
       " 'brown': 119,\n",
       " 'cloud': 178,\n",
       " 'chief': 158,\n",
       " 'race': 765,\n",
       " 'moment': 609,\n",
       " 'brings': 114,\n",
       " 'castle': 141,\n",
       " 'security': 860,\n",
       " 'trump': 995,\n",
       " 'bass': 77,\n",
       " 'prisoner': 738,\n",
       " 'october': 651,\n",
       " 'charity': 153,\n",
       " 'energy': 307,\n",
       " 'arsenal': 50,\n",
       " 'gains': 387,\n",
       " 'holdings': 449,\n",
       " 'evening': 314,\n",
       " 'shares': 877,\n",
       " 'greece': 421,\n",
       " 'video': 1022,\n",
       " 'mary': 582,\n",
       " 'let': 537,\n",
       " 'rule': 833,\n",
       " 'hold': 448,\n",
       " 'push': 760,\n",
       " 'center': 145,\n",
       " 'island': 491,\n",
       " 'named': 627,\n",
       " 'eye': 327,\n",
       " 'pancreas': 674,\n",
       " 'air': 19,\n",
       " 'dr': 280,\n",
       " 'church': 164,\n",
       " 'malaysia': 570,\n",
       " 'pre': 726,\n",
       " 'syrian': 951,\n",
       " 'rating': 775,\n",
       " 'god': 409,\n",
       " 'director': 268,\n",
       " 'underestimated': 1006,\n",
       " 'closer': 175,\n",
       " 'scotland': 851,\n",
       " 'preview': 734,\n",
       " 'hit': 446,\n",
       " 'dancing': 233,\n",
       " 'api': 43,\n",
       " 'young': 1069,\n",
       " ...}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ivocab = tf_vectorizer.get_feature_names()\n",
    "vocab = {}\n",
    "for i,w in enumerate(ivocab):\n",
    "    vocab[w]=i\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['worcester', 'breakfast', 'club', 'veterans', 'gives', 'orders']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabSize = len(ivocab)\n",
    "[x for x in cleantitres[0].split() if x in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1059, 110, 179, 1020, 405, 666]\n",
      "[1059, 110, 179, 1020, 405]\n",
      "[110, 179, 1020, 405, 666]\n"
     ]
    }
   ],
   "source": [
    "doc0 = [vocab[x] for x in cleantitres[0].split() if x in vocab]\n",
    "print(doc0)\n",
    "print(doc0[:len(doc0)-1])\n",
    "print(doc0[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "vectors = np.array([[vocab[x] for x in cleantitres[0].split() if x in vocab]]).reshape(-1)\n",
    "one_hot_vectors = np.eye(vocabSize)[vectors]\n",
    "print(one_hot_vectors[0])\n",
    "print(sum(one_hot_vectors[0]))\n",
    "print(one_hot_vectors[0][1059])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter= 1 Error= 0.811247849538\n",
      "Iter= 10000 Error= 0.500000190629\n",
      "Iter= 20000 Error= 0.500000131117\n",
      "Iter= 30000 Error= 0.500000101904\n",
      "Iter= 40000 Error= 0.500000083907\n",
      "Iter= 50000 Error= 0.500000071493\n",
      "Iter= 60000 Error= 0.500000062318\n",
      "Iter= 70000 Error= 0.500000055212\n",
      "Iter= 80000 Error= 0.500000049516\n",
      "Iter= 90000 Error= 0.500000044829\n",
      "Iter= 100000 Error= 0.500000040891\n",
      "Iter= 110000 Error= 0.500000037524\n",
      "Iter= 120000 Error= 0.500000034605\n",
      "Iter= 130000 Error= 0.500000032043\n",
      "Iter= 140000 Error= 0.500000029769\n",
      "Iter= 150000 Error= 0.500000027733\n",
      "Iter= 160000 Error= 0.500000025893\n",
      "Iter= 170000 Error= 0.500000024217\n",
      "Iter= 180000 Error= 0.50000002268\n",
      "Iter= 190000 Error= 0.50000002126\n",
      "Iter= 200000 Error= 0.500000019941\n",
      "----\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.5         0.50000002  0.49999991 ...,  0.50000022  0.50000013\n",
      "   0.50000002]\n",
      " [ 0.49999999  0.50000004  0.4999999  ...,  0.50000028  0.50000015\n",
      "   0.50000003]\n",
      " [ 0.49999999  0.50000003  0.49999992 ...,  0.50000014  0.50000009\n",
      "   0.49999999]\n",
      " [ 0.49999998  0.50000004  0.49999988 ...,  0.50000028  0.50000016\n",
      "   0.50000002]\n",
      " [ 0.49999999  0.50000003  0.49999996 ...,  0.5000001   0.50000004\n",
      "   0.49999999]]\n",
      "[180 756 446 446 446]\n"
     ]
    }
   ],
   "source": [
    "simplewordembeddings = SimpleNN(vocabSize,5,vocabSize)\n",
    "simplewordembeddings.run(one_hot_vectors[:len(one_hot_vectors)-1],one_hot_vectors[1:],n_iter=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 110  179 1020  405  666]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(one_hot_vectors[1:], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter= 1 Error= 0.816596176245\n",
      "Iter= 10000 Error= 0.500000224025\n",
      "Iter= 20000 Error= 0.500000147028\n",
      "Iter= 30000 Error= 0.500000109171\n",
      "Iter= 40000 Error= 0.500000085472\n",
      "Iter= 50000 Error= 0.500000068433\n",
      "Iter= 60000 Error= 0.500000054804\n",
      "Iter= 70000 Error= 0.500000042702\n",
      "Iter= 80000 Error= 0.500000030526\n",
      "Iter= 90000 Error= 0.500000015966\n",
      "Iter= 100000 Error= 0.499999993097\n",
      "Iter= 110000 Error= 0.499999928593\n",
      "Iter= 120000 Error= 0.00749440447818\n",
      "Iter= 130000 Error= 0.00428838434892\n",
      "Iter= 140000 Error= 0.00358504821551\n",
      "Iter= 150000 Error= 0.00323612904287\n",
      "Iter= 160000 Error= 0.00301833513451\n",
      "Iter= 170000 Error= 0.00286590173212\n",
      "Iter= 180000 Error= 0.00275156320721\n",
      "Iter= 190000 Error= 0.00266171016609\n",
      "Iter= 200000 Error= 0.00258869199182\n",
      "----\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[[ 0.0011008   0.0011008   0.0011008  ...,  0.0011008   0.00110081\n",
      "   0.00110081]\n",
      " [ 0.00110223  0.00110223  0.00110223 ...,  0.00110223  0.00110224\n",
      "   0.00110224]\n",
      " [ 0.0011008   0.0011008   0.0011008  ...,  0.0011008   0.00110081\n",
      "   0.00110081]\n",
      " [ 0.00110101  0.00110101  0.00110101 ...,  0.00110102  0.00110102\n",
      "   0.00110102]\n",
      " [ 0.00110159  0.00110159  0.00110159 ...,  0.00110159  0.0011016\n",
      "   0.0011016 ]]\n",
      "[ 110 1020 1020  110 1020]\n"
     ]
    }
   ],
   "source": [
    "simplewordembeddings = SimpleNN(vocabSize,5,vocabSize)\n",
    "simplewordembeddings.run(one_hot_vectors[:len(one_hot_vectors)-1],one_hot_vectors[1:],n_iter=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 110  179 1020  405  666]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(one_hot_vectors[1:], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breakfast\n",
      "[-14.88983354   8.56395537 -15.17251454 -14.3849577  -14.9929134 ]\n"
     ]
    }
   ],
   "source": [
    "print(ivocab[110])\n",
    "print(simplewordembeddings._syn0[110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "veterans\n",
      "[-14.65993327  10.46939782 -14.8943817  -16.02371326 -15.44001003]\n"
     ]
    }
   ],
   "source": [
    "print(ivocab[1020])\n",
    "print(simplewordembeddings._syn0[1020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['veterans',\n",
       " 'saluted',\n",
       " 'worcester',\n",
       " 's',\n",
       " 'first',\n",
       " 'ever',\n",
       " 'breakfast',\n",
       " 'club',\n",
       " 'for',\n",
       " 'ex',\n",
       " 'soldiers',\n",
       " 'which',\n",
       " 'won',\n",
       " 'over',\n",
       " 'hearts',\n",
       " 'minds',\n",
       " 'and',\n",
       " 'bellies',\n",
       " 'the',\n",
       " 'worcester']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=sum([sentense2cleanTokens(x).split() for x in content],[])\n",
    "print(len(words))\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 26326], ('the', 22122), ('to', 11320), ('and', 11156), ('of', 9891)]\n",
      "Sample data [2697, 0, 8985, 8, 55, 401, 3055, 654, 7, 1730]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 10000\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    #add most frequent (without taking account eats\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['veterans', 'UNK', 'worcester', 's', 'first', 'ever', 'breakfast', 'club']\n",
      "\n",
      "with num_skips = 2 and skip_window = 1:\n",
      "    batch: ['breakfast', 'breakfast', 'club', 'club', 'for', 'for', 'ex', 'ex']\n",
      "    labels: ['ever', 'club', 'for', 'breakfast', 'club', 'ex', 'soldiers', 'for']\n",
      "\n",
      "with num_skips = 4 and skip_window = 2:\n",
      "    batch: ['club', 'club', 'club', 'club', 'for', 'for', 'for', 'for']\n",
      "    labels: ['ex', 'ever', 'for', 'breakfast', 'breakfast', 'soldiers', 'club', 'ex']\n"
     ]
    }
   ],
   "source": [
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "for num_skips, skip_window in [(2, 1), (4, 2)]:\n",
    "    data_index = 5\n",
    "    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "    # Input data.\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    # Variables.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "    # Optimizer.\n",
    "    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "    # This is because the embeddings are defined as a variable quantity and the\n",
    "    # optimizer's `minimize` method will by default modify all variable quantities \n",
    "    # that contribute to the tensor it is passed.\n",
    "    # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.711872\n",
      "Nearest to are: athletic, bonds, thrown, andujar, worldwide, processed, exactly, states,\n",
      "Nearest to only: likely, lists, inaugural, indecent, devopssummit, griffin, range, pressing,\n",
      "Nearest to would: refuse, cs, auditor, ringgit, chip, interior, iran, tensas,\n",
      "Nearest to years: clinton, windows, accordingly, size, morning, pdp, materials, functional,\n",
      "Nearest to our: acquiring, rising, warrant, remark, longtime, collar, considerably, emergence,\n",
      "Nearest to there: musical, this, hypertension, possibility, glyphosate, door, retailers, radio,\n",
      "Nearest to of: gross, complements, browning, chinese, inspectors, addition, visibility, feelings,\n",
      "Nearest to get: reference, herein, join, protect, expect, applying, authentic, documented,\n",
      "Nearest to what: young, poseidon, convenience, workforce, usaconsultants, scrap, mouth, twenty,\n",
      "Nearest to company: corporate, killings, darwin, applying, signal, holyrood, commit, shield,\n",
      "Nearest to back: sole, standardised, characteristics, open, demonstration, eligible, too, completing,\n",
      "Nearest to her: crane, legislation, himself, try, nvidia, batch, partnerships, demo,\n",
      "Nearest to had: arrangements, bash, handy, iiroc, affiliated, goods, helmet, referendum,\n",
      "Nearest to so: most, thc, schoch, winery, norfolk, sunlight, bartlett, assistance,\n",
      "Nearest to to: northern, dd, continental, albuquerque, thursday, nancy, reflex, stroke,\n",
      "Nearest to into: selling, fiber, commodities, qualifier, generate, victory, questioned, regime,\n",
      "Average loss at step 2000: 4.152384\n",
      "Average loss at step 4000: 3.888412\n",
      "Average loss at step 6000: 3.810961\n",
      "Average loss at step 8000: 3.559640\n",
      "Average loss at step 10000: 3.519350\n",
      "Nearest to are: were, re, is, worldwide, have, relation, was, complexity,\n",
      "Nearest to only: lists, likely, accelerated, very, ezpaycheck, pressing, stroke, db,\n",
      "Nearest to would: will, could, ll, can, tensas, var, ringgit, should,\n",
      "Nearest to years: windows, clinton, year, vending, times, morning, functional, shut,\n",
      "Nearest to our: their, courtesy, acquiring, the, your, its, established, secrets,\n",
      "Nearest to there: it, still, musical, she, hypertension, merely, this, he,\n",
      "Nearest to of: canadian, chinese, gross, keuchel, diversified, drew, for, massive,\n",
      "Nearest to get: reference, protect, go, expect, join, watch, change, documented,\n",
      "Nearest to what: it, how, halifax, mime, deception, hottest, native, scrap,\n",
      "Nearest to company: reluctance, killings, corporate, literally, europe, darwin, plants, combined,\n",
      "Nearest to back: open, sole, enjoying, standardised, too, fight, htm, characteristics,\n",
      "Nearest to her: crane, my, sibling, his, chamber, their, combines, the,\n",
      "Nearest to had: have, has, blamed, was, bradmark, affiliated, arrangements, rays,\n",
      "Nearest to so: norfolk, severity, stemming, coupled, thc, fed, dublin, headaches,\n",
      "Nearest to to: can, dd, wording, swaraj, cannot, will, northern, nancy,\n",
      "Nearest to into: past, generate, selling, kanye, fiber, qualifier, parameters, russian,\n",
      "Average loss at step 12000: 3.520569\n",
      "Average loss at step 14000: 3.378738\n",
      "Average loss at step 16000: 3.355802\n",
      "Average loss at step 18000: 3.354563\n",
      "Average loss at step 20000: 3.263245\n",
      "Nearest to are: were, re, is, worldwide, have, complexity, be, was,\n",
      "Nearest to only: lists, likely, accelerated, resulted, very, best, ezpaycheck, bo,\n",
      "Nearest to would: will, could, ll, should, can, might, tensas, shall,\n",
      "Nearest to years: months, times, days, windows, year, vending, past, weeks,\n",
      "Nearest to our: their, secrets, your, courtesy, acquiring, its, the, beaches,\n",
      "Nearest to there: she, musical, it, merely, we, still, they, hypertension,\n",
      "Nearest to of: chinese, gross, fourteen, canadian, huskies, alongside, complements, rm,\n",
      "Nearest to get: expect, protect, go, reference, watch, change, join, come,\n",
      "Nearest to what: how, mime, it, when, halifax, departure, whether, that,\n",
      "Nearest to company: reluctance, literally, plants, killings, combined, world, goal, corporate,\n",
      "Nearest to back: open, htm, sole, standardised, eligible, detailed, up, forward,\n",
      "Nearest to her: his, my, crane, citi, their, your, sibling, nordic,\n",
      "Nearest to had: have, has, bradmark, was, affiliated, became, blamed, aside,\n",
      "Nearest to so: norfolk, pieces, stemming, severity, coupled, headaches, else, literally,\n",
      "Nearest to to: eagles, dd, will, salvaggio, please, would, must, reflex,\n",
      "Nearest to into: generate, through, qualifier, median, commodities, past, key, kanye,\n",
      "Average loss at step 22000: 3.233712\n",
      "Average loss at step 24000: 3.235165\n",
      "Average loss at step 26000: 3.181455\n",
      "Average loss at step 28000: 3.147742\n",
      "Average loss at step 30000: 3.148333\n",
      "Nearest to are: were, re, have, worldwide, complexity, agility, loves, relation,\n",
      "Nearest to only: lists, likely, best, accelerated, resulted, bo, pkr, greatest,\n",
      "Nearest to would: will, could, ll, should, might, can, may, tensas,\n",
      "Nearest to years: months, times, decades, days, weeks, past, windows, hours,\n",
      "Nearest to our: their, secrets, your, courtesy, the, acquiring, its, significantly,\n",
      "Nearest to there: it, musical, maiden, we, here, merely, she, morison,\n",
      "Nearest to of: gross, influences, rail, coalition, stretch, diversified, for, luncheon,\n",
      "Nearest to get: watch, protect, expect, change, begin, see, come, receive,\n",
      "Nearest to what: how, treat, that, departure, why, whether, claim, incredibly,\n",
      "Nearest to company: reluctance, world, literally, uncertain, killings, sail, plants, goal,\n",
      "Nearest to back: open, htm, forward, detailed, playoffs, standardised, sole, into,\n",
      "Nearest to her: his, my, crane, citi, their, your, waterfront, bids,\n",
      "Nearest to had: have, has, became, blamed, poet, helmet, demonstrating, affiliated,\n",
      "Nearest to so: literally, pieces, everybody, too, sure, headaches, stemming, else,\n",
      "Nearest to to: swaraj, dd, rare, seamless, jacquard, can, monster, cells,\n",
      "Nearest to into: through, median, generate, kanye, electronic, off, removing, past,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 30001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver = tf.train.Saver()\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = generate_batch(\n",
    "          batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "        # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s,' % (log, close_word)\n",
    "                print(log)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "    #saver.save(session, 'modelW2V.vec')\t\n",
    "    #writer_ = tf.summary.FileWriter('./', session.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
