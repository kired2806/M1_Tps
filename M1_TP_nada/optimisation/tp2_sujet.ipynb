{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 - scaling, différences finies et méthode du gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "from scipy.optimize import minimize \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 1 - intérêt de la mise à l'échelle (\"scaling\") en optimisation\n",
    "\n",
    "Considérons la fonction de deux variables réelles suivante :\n",
    "$$\n",
    "f_{M}(x,y) = (x-M)^4+(x-M)^2+\\left(y-\\dfrac{1}{M}\\right)^4+\\left(y-\\dfrac{1}{M}\\right)^2.\n",
    "$$\n",
    "\n",
    "+ En utilisant la fonction *minimize* de la bibliothèque *scipy.optimize*, chercher une solution approchée $(x^*,y^*)$ de la minimisation de cette fonction $f_{M}$ en partant du point $(1,1)$ et en utilisant l'option *Nelder-Mead*.\n",
    "Observer ce qui se passe lorsque $M >> 1$, par exemple $M = 10^9$.\n",
    "\n",
    "+ Ensuite, construire la fonction $f^S_M$ définie par \n",
    "$$\n",
    "f^S_{M}(u,v) = f_M\\left(Mu,\\dfrac{v}{M}\\right)\n",
    "$$\n",
    "et chercher une solution approchée $(u^*,v^*)$ de la minimisation de cette fonction $f^S_{M}$ en partant du point $\\left(\\dfrac{1}{M},M\\right)$.\n",
    "\n",
    "+ Comparer $(x^*,y^*)$ et $\\left(M u^*,\\dfrac{v^*}{M}\\right)$.\n",
    "\n",
    "Remarque : on ajoutera *options={'disp': True, 'maxiter':1000}* dans les appels à *minimize*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10**9\n",
    "f = lambda a : (a[0]-M)**2 + (a[0]-M)**4 + (a[1]-1/M)**2 + (a[1]-1/M)**4\n",
    "fs = lambda b : (M*(b[0]-1))**2 + (M*(b[0]-1))**4 + ((b[1]-1)/M)**2 + ((b[1]-1)/M)**4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 260\n",
      "         Function evaluations: 467\n",
      "999999999.9999704 1.2911399266203106e-05\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([1,1])\n",
    "\n",
    "res_f = minimize(f, x0, method='Nelder-Mead',options={'disp': True, 'maxiter':1000})\n",
    "\n",
    "(x_sol_f,y_sol_f) = np.array(res_f.x) \n",
    "\n",
    "print(x_sol_f,y_sol_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 309\n",
      "         Function evaluations: 557\n",
      "1000000000.0 1.0000110451478519e-09\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([1/M,M])\n",
    "\n",
    "res_fs = minimize(fs, x0, method='Nelder-Mead',options={'disp': True, 'maxiter':1000})\n",
    "\n",
    "(u_sol_fs,v_sol_fs) = np.array(res_fs.x) \n",
    "\n",
    "print(M*u_sol_fs,v_sol_fs/M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la solution est plus exacte après la mise en échelle au coup de plus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 2 - approximation d'un gradient et d'une matrice hessienne par différences finies\n",
    "\n",
    "On se rappelle qu'une approximation de la dérivée $f'(x)$ d'une fonction $f$ définie, continue et dérivable sur $\\mathbb{R}$ est simplement, pour $h$ petit, \n",
    "$a_h := \\dfrac{f(x+h)-f(x)}{h}.$\n",
    "On a alors $|a_h-f'(x)| = O(h)$.\n",
    "\n",
    "On généralise aisément cela au cas d'une fonction $f$ à valeurs réelles définie sur $\\mathbb{R}^n$. En effet, si on note $e_i$ le $i$-ème vecteur de la base canonique de $\\mathbb{R}^n$, la $i$-ème composante de $\\nabla f(x)$ est approchée par\n",
    "$$\n",
    "\\dfrac{f(x+he_i)-f(x)}{h}.\n",
    "$$\n",
    "\n",
    "De manière analogue, on peut approcher la composante $(i,j)$ de la matrice hessienne de $f$ en $x$ par \n",
    "$$\n",
    "\\dfrac{f(x+he_i+he_j)-f(x+he_i)-f(x+he_j)+f(x)}{h^2}.\n",
    "$$\n",
    "\n",
    "+ Ecrire une fonction **FDgrad(f,x,h)** qui calcule une approximation du gradient de la fonction $f$ au point $x$ en prenant un pas $h$.\n",
    "\n",
    "+ Tester cette fonction pour une fonction $f$ dont vous connaissez le gradient (par exemple la fonction de Rosenbrock) et observez son comportement lorsque $h$ varie dans l'ensemble $\\left\\{10^{-k}, k = 1, 2, ... 16\\right\\}$. Pour ce faire, on pourra faire afficher l'erreur commise par rapport au gradient exact.\n",
    "\n",
    "+ Ecrire une fonction **FDhess(f,x,h)** qui calcule une approximation de la matrice hessienne de la fonction $f$ au point $x$ en prenant un pas $h$.\n",
    "\n",
    "+ Tester cette fonction pour une fonction $f$ dont vous connaissez la matrice hessienne et observez son comportement lorsque $h$ varie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 3 - méthode du gradient à pas fixe\n",
    "\n",
    "Lorsqu'on cherche à minimiser une fonction $f$ définie sur $\\mathbb{R}^n$, l'une des méthodes les plus élémentaires est la méthode de gradient. Elle consiste à construire une suite d'itérés $(x_k)_{k\\in \\mathbb{N}}$ de la manière suivante : partant d'un $x_0 \\in \\mathbb{R}^n$, pour tout $k \\in \\mathbb{N}$,\n",
    "$$\n",
    "x_{k+1} = x_k+\\alpha_k d_k, \\mbox{ avec } d_k = -\\nabla f(x_k).\n",
    "$$\n",
    "\n",
    "Comme $d_k = -\\nabla f(x_k)$ est une direction de descente ($\\nabla f(x_k)^\\top d_k < 0$), la suite ci-dessus fait nécessairement décroître $f$, quitte à prendre un pas constant $\\alpha_k  = \\alpha$ très petit.\n",
    "\n",
    "+ Ecrire une fonction **GPF(x0,f,df,alpha,itmax)** qui fournit la suite décrite ci-dessus pour minimiser une fonction **f**, en partant du vecteur initial **x0** et en utilisant le pas fixe stocké dans **alpha**, sans faire plus de **itmax** itérations. L'argument **df** est le nom de la fonction qui calcule le gradient de **f**.\n",
    "\n",
    "+ Pour mettre au point cette fonction, on commencera par faire un nombre fixe d'itérations et on la testera sur la fonction de Rosenbrock dont on connaît le minimum :\n",
    "$$\n",
    "p(x_1^2-x_2)^2+(1-x_1)^2.\n",
    "$$\n",
    "On pourra faire les essais avec $p = 10$.\n",
    "Les points de départ standards pour cette fonction sont $(-1.2, 1)$ et $(6.39,-0.221)$.\n",
    "Le temps de déboguer, on prendra **itmax** = 100, puis ensuite on le mettra à 1000.\n",
    "Il faudra prendre le temps de choisir les pas fixes adaptés à chacun des deux points de départ.\n",
    "\n",
    "Afin de constater que la méthode fonctionne, \n",
    "+ on fera un tracé des itérés construits, superposé aux contours de la fonction minimisée,\n",
    "+ on fera un tracé en échelle logarithmique de la décroissance de la fonction $f$ en fonction des itérations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 4 - recherche linéaire d'Armijo\n",
    "\n",
    "On a observé ci-dessus que le pas fixe n'est pas forcément très pratique. De fait, il est rarement utilisé en pratique.\n",
    "\n",
    "On utilise plutôt une **recherche linéaire** pour trouver un pas $\\alpha_k$ adapté à chaque itération. Par souci de simplicité, on décrit ici une recherche linéaire basée sur \n",
    "+ la règle d'Armijo : on cherche un pas $\\alpha_k$ qui vérifie la condition de décroissance suffisante\n",
    "$$\n",
    "f(x_k+\\alpha_k d_k) \\leq f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top d_k,\n",
    "$$\n",
    "avec $c_1 \\in \\left]0,\\dfrac{1}{2}\\right[$. Une valeur classique $c_1$ est $10^{-4}$ ; \n",
    "+ un \"backtracking\" : on cherche $\\alpha_k$ parmi les $\\left\\{\\dfrac{1}{\\beta^k}, k \\in\\mathbb{N}\\right\\}$.\n",
    "\n",
    "On se propose de coder l'algorithme de recherche linéaire suivant dans une fonction \n",
    "**RLA(xk,dk,alpha0,f,ps)** : étant donnés l'itéré courant $x_k$, la direction de descente $d_k$, la fonction $f$ à minimiser et le produit scalaire **ps** contenant $\\nabla f(x_k)^\\top d_k$, on initialise le pas $\\alpha$ à $\\alpha_0$, puis tant que la condition d'Armijo n'est pas satisfaite, on divise $\\alpha$ par $2$. La fonction **RLA** doit retourner la valeur du pas ainsi obtenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 5 - méthode de gradient globalisé\n",
    "\n",
    "A partir de la fonction GPF(x0,f,df,alpha,itmax), écrire une fonction **GRLA(x0,f,df,alpha0,itmax)** qui fournit la suite \n",
    "$$\n",
    "x_{k+1} = x_k+\\alpha_k d_k, \\mbox{ avec } d_k = -\\nabla f(x_k).\n",
    "$$\n",
    "pour minimiser une fonction **f**, en partant du vecteur initial **x0**, mais cette fois en utilisant la recherche linéaire d'Armijo codée ci-dessus, initialisée à **alpha0** pour ajuster le pas $\\alpha_k$ à chaque itération. \n",
    "\n",
    "On fera les mêmes tests que dans l'exercice 3, en initialisant la recherche linéaire avec $\\alpha_0 = 1$ et on ajoutera le tracé de l'évolution du pas $\\alpha$ au cours des itérations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 6 - amélioration des critères d'arrêt\n",
    "\n",
    "Maintenant que la fonction **GRLA** est au point, on peut essayer de mettre en place des critères d'arrêt plus réalistes :  \n",
    "$$\n",
    "k > itmax, \n",
    "\\quad\\mbox{ou}\\quad\n",
    "\\max_{1\\leq i\\leq n}\\left|\n",
    "\\dfrac{(\\nabla f(x_k))_i (x_{k+1})_i}{f(x_{k+1})}\n",
    "\\right| \n",
    "\\leq \\varepsilon_g, \n",
    "\\quad\\mbox{ou}\\quad\n",
    "\\max_{1\\leq i\\leq n}\\left|\n",
    "\\dfrac{(x_{k+1})_i-(x_{k})_i}{(x_{k+1})_i}\n",
    "\\right| \n",
    "\\leq \\varepsilon_x.\n",
    "$$\n",
    "\n",
    "Ainsi, à partir de GRLA(x0,f,df,alpha0,itmax), construire une fonction **GRLA2(x0,f,df,alpha0,itmax,epsilg,epsilx)** qui remplace la boucle *for* en une boucle *while* avec les critères d'arrêt indiqués ci-dessus.\n",
    "\n",
    "Refaire les mêmes tests que précédemment, en commençant avec $\\varepsilon_g = \\varepsilon_x = 10^{-4}$.\n",
    "\n",
    "Remarque : en pratique, pour éviter les problèmes lorsque l'un des $(x_{k+1})_i$ ou $f(x_{k+1})$ devient nuls, on utilise plutôt les tests robustes suivants : \n",
    "$$\n",
    "k > itmax, \n",
    "\\quad\\mbox{ou}\\quad\n",
    "\\max_{1\\leq i\\leq n}\\left|\n",
    "\\dfrac{(\\nabla f(x_k))_i \\max\\{|(x_{k+1})_i|,typx_i\\}}{\\max\\{|f(x_{k+1})|,typf\\}}\n",
    "\\right| \n",
    "\\leq \\varepsilon_g, \n",
    "\\quad\\mbox{ou}\\quad\n",
    "\\max_{1\\leq i\\leq n}\\left|\n",
    "\\dfrac{(x_{k+1})_i-(x_{k})_i}{\\max\\{|(x_{k+1})_i|,typx_i\\}}\n",
    "\\right| \n",
    "\\leq \\varepsilon_x.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
